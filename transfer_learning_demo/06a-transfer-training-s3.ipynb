{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/saturn_logo.png\" width=\"300\" />\n",
    "\n",
    "We don't need to run all of Notebook 5 again, we'll just call `setup2.py` in the next chunk to get ourselves back to the right state. This also includes the reindexing work from Notebook 5, and a couple of visualization functions that we'll talk about later.\n",
    "\n",
    "***\n",
    "**Note: This notebook assumes you have an S3 bucket where you can store your model performance statistics.**  \n",
    "If you don't have access to an S3 bucket, but would still like to train your model and review results, please visit [Notebook 6b](06b-transfer-training-local.ipynb) and [Notebook 7](07-learning-results.ipynb) to see detailed examples of how you can do that.\n",
    "***\n",
    "\n",
    "## Connect to Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-23T17:22:25.910750Z",
     "iopub.status.busy": "2021-02-23T17:22:25.910490Z",
     "iopub.status.idle": "2021-02-23T17:22:27.021894Z",
     "shell.execute_reply": "2021-02-23T17:22:27.021388Z",
     "shell.execute_reply.started": "2021-02-23T17:22:25.910684Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dask-saturn:Cluster is ready\n",
      "INFO:dask-saturn:Registering default plugins\n",
      "INFO:dask-saturn:{'tcp://10.0.15.16:38589': {'status': 'repeat'}, 'tcp://10.0.3.145:39377': {'status': 'repeat'}, 'tcp://10.0.8.9:36789': {'status': 'repeat'}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<b>Cluster Dashboard links</b>\n",
       "<ul>\n",
       "<li><a href=\"https://d-steph-pytorch-training-90e6119a500640599ff558a22c25098d.internal.saturnenterprise.io/status\" target=\"_blank\">CPU dashboard</a></li>\n",
       "<li><a href=\"https://d-steph-pytorch-training-90e6119a500640599ff558a22c25098d.internal.saturnenterprise.io/individual-gpu-utilization\" target=\"_blank\">GPU utilization</a></li>\n",
       "<li><a href=\"https://d-steph-pytorch-training-90e6119a500640599ff558a22c25098d.internal.saturnenterprise.io/individual-gpu-memory\" target=\"_blank\">GPU memory</a></li>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run -i ../tools/setup2.py\n",
    "\n",
    "display(HTML(gpu_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-23T17:22:54.119061Z",
     "iopub.status.busy": "2021-02-23T17:22:54.118825Z",
     "iopub.status.idle": "2021-02-23T17:22:54.410995Z",
     "shell.execute_reply": "2021-02-23T17:22:54.410478Z",
     "shell.execute_reply.started": "2021-02-23T17:22:54.119035Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-23T17:22:54.634684Z",
     "iopub.status.busy": "2021-02-23T17:22:54.634495Z",
     "iopub.status.idle": "2021-02-23T17:22:54.639435Z",
     "shell.execute_reply": "2021-02-23T17:22:54.639004Z",
     "shell.execute_reply.started": "2021-02-23T17:22:54.634663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://d-steph-pytorch-training-90e6119a500640599ff558a22c25098d.main-namespace:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='https://d-steph-pytorch-training-90e6119a500640599ff558a22c25098d.internal.saturnenterprise.io' target='_blank'>https://d-steph-pytorch-training-90e6119a500640599ff558a22c25098d.internal.saturnenterprise.io</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>96</li>\n",
       "  <li><b>Memory: </b>382.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.0.10.44:8786' processes=3 threads=96, memory=382.50 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to do some learning! \n",
    "\n",
    "## Model Parameters\n",
    "\n",
    "Aside from the Special Elements noted below, we can write this section essentially the same way we write any other PyTorch training loop. \n",
    "* Cross Entropy Loss for our loss function\n",
    "* SGD (Stochastic Gradient Descent) for our optimizer\n",
    "\n",
    "We have two stages in this process, as well - training and evaluation. We run the training set completely using batches of 100 before we move to the evaluation step, where we run the eval set completely also using batches of 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the training workflow function shown will be very familiar for users of PyTorch. However, there are a couple of elements that are different.\n",
    "\n",
    "### 1. Tensorboard Writer\n",
    "\n",
    "We're using Tensorboard to monitor the model's performance, so we'll create a SummaryWriter object in our training function, and use that to write out statistics and sample image classifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model to GPU Resources\n",
    "\n",
    "```\n",
    "device = torch.device(0)\n",
    "net = models.resnet50(pretrained=True)\n",
    "model = net.to(device)\n",
    "```\n",
    "\n",
    "We need to make sure our model is assigned to a GPU resource- here we do it one time before the training loops begin. We will also assign each image and its label to a GPU resource within the training and evaluation loops.\n",
    "\n",
    "\n",
    "### 3. DDP Wrapper\n",
    "```\n",
    "model = DDP(model)\n",
    "```\n",
    "\n",
    "And finally, we need to enable the DistributedDataParallel framework. To do this, we are using the `DDP()` wrapper around the model, which is short for the PyTorch function `torch.nn.parallel.DistributedDataParallel`. There is a lot to know about this, but for our purposes the important thing is to understand that this allows the model training to run in parallel on our cluster. https://pytorch.org/docs/stable/notes/ddp.html\n",
    "\n",
    "\n",
    "\n",
    "> **Discussing DDP**   \n",
    "It may be interesting for you to know what DDP is really doing under the hood: for a detailed discussion and more tips about this same workflow, you can visit our blog to read more! [https://www.saturncloud.io/s/combining-dask-and-pytorch-for-better-faster-transfer-learning/](https://www.saturncloud.io/s/combining-dask-and-pytorch-for-better-faster-transfer-learning/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "# Training time!\n",
    "Our whole training process is going to be contained in one function, here named `run_transfer_learning`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Modeling Functions\n",
    "\n",
    "Setting these pretty basic steps into a function just helps us ensure perfect parity between our train and evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-23T16:55:48.624364Z",
     "iopub.status.busy": "2021-02-23T16:55:48.624211Z",
     "iopub.status.idle": "2021-02-23T16:55:48.628712Z",
     "shell.execute_reply": "2021-02-23T16:55:48.628270Z",
     "shell.execute_reply.started": "2021-02-23T16:55:48.624346Z"
    }
   },
   "outputs": [],
   "source": [
    "def iterate_model(inputs, labels, model, device):\n",
    "    # Pass items to GPU\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Run model iteration\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Format results\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    perct = [torch.nn.functional.softmax(el, dim=0)[i].item() for i, el in zip(preds, outputs)]\n",
    "    \n",
    "    return inputs, labels, outputs, preds, perct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-23T16:55:49.017918Z",
     "iopub.status.busy": "2021-02-23T16:55:49.017760Z",
     "iopub.status.idle": "2021-02-23T16:55:49.034164Z",
     "shell.execute_reply": "2021-02-23T16:55:49.033711Z",
     "shell.execute_reply.started": "2021-02-23T16:55:49.017899Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_transfer_learning(bucket, prefix, train_pct, batch_size, \n",
    "                          n_epochs, base_lr, imagenetclasses, \n",
    "                          n_workers = 1, subset = False):\n",
    "    '''Load basic Resnet50, run transfer learning over given epochs.\n",
    "    Uses dataset from the path given as the pool from which to take the \n",
    "    training and evaluation samples.'''\n",
    "    \n",
    "    worker_rank = int(dist.get_rank())\n",
    "    \n",
    "    # Set results writer\n",
    "    writer = SummaryWriter(f's3://pytorchtraining/pytorch_bigbatch/learning_worker{worker_rank}')\n",
    "    executor = ThreadPoolExecutor(max_workers=64)\n",
    "    \n",
    "    # --------- Format model and params --------- #\n",
    "    device = torch.device(\"cuda\")\n",
    "    net = models.resnet50(pretrained=True) # True means we start with the imagenet version\n",
    "    model = net.to(device)\n",
    "    model = DDP(model)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda()    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9)\n",
    "\n",
    "    # --------- Retrieve data for training and eval --------- #\n",
    "    whole_dataset = prepro_batches(bucket, prefix)\n",
    "    new_class_to_idx = {x: int(replace_label(x, imagenetclasses)[1]) for x in whole_dataset.classes}\n",
    "    whole_dataset.class_to_idx = new_class_to_idx\n",
    "    \n",
    "    train, val = get_splits_parallel(train_pct, whole_dataset, batch_size=batch_size, subset = subset, workers = n_workers)\n",
    "    dataloaders = {'train' : train, 'val': val}\n",
    "\n",
    "    # --------- Start iterations --------- #\n",
    "    count = 0\n",
    "    t_count = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        agg_loss = []\n",
    "        agg_loss_t = []\n",
    "        \n",
    "        agg_cor = []\n",
    "        agg_cor_t = []\n",
    "    # --------- Training section --------- #    \n",
    "        model.train()  # Set model to training mode\n",
    "        for inputs, labels in dataloaders[\"train\"]:\n",
    "            dt = datetime.datetime.now().isoformat()\n",
    "\n",
    "            inputs, labels, outputs, preds, perct = iterate_model(inputs, labels, model, device)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "            \n",
    "            # Track statistics\n",
    "            for param_group in optimizer.param_groups:\n",
    "                current_lr = param_group['lr']\n",
    "                \n",
    "            agg_loss.append(loss.item())\n",
    "            agg_cor.append(correct)\n",
    "\n",
    "            if ((count % 25) == 0): \n",
    "                future = executor.submit(\n",
    "                    writer.add_hparams(\n",
    "                        hparam_dict = {'lr': current_lr, 'bsize': batch_size, 'worker':worker_rank},\n",
    "                        metric_dict = {'correct': correct,'loss': loss.item()},\n",
    "                        name = 'train-iter',\n",
    "                        global_step=count)\n",
    "                )\n",
    "\n",
    "            # Save a matplotlib figure showing a small sample of actual preds for spot check\n",
    "            # Functions used here are in setup2.py\n",
    "            if ((count % 50) == 0):\n",
    "                future = executor.submit(\n",
    "                    writer.add_figure(\n",
    "                        'predictions vs. actuals, training',\n",
    "                        plot_classes_preds(model, inputs, labels, preds, perct, imagenetclasses),\n",
    "                        global_step=count\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "    # --------- Evaluation section --------- #   \n",
    "        with torch.no_grad():\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            for inputs_t, labels_t in dataloaders[\"val\"]:\n",
    "                dt = datetime.datetime.now().isoformat()\n",
    "                \n",
    "                inputs_t, labels_t, outputs_t, pred_t, perct_t = iterate_model(inputs_t, labels_t, model, device)\n",
    "\n",
    "                loss_t = criterion(outputs_t, labels_t)\n",
    "                correct_t = (pred_t == labels_t).sum().item()\n",
    "            \n",
    "                t_count += 1\n",
    "\n",
    "                # Track statistics\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    current_lr = param_group['lr']\n",
    "                    \n",
    "                agg_loss_t.append(loss_t.item())\n",
    "                agg_cor_t.append(correct_t)\n",
    "\n",
    "                if ((t_count % 25) == 0):\n",
    "                    future = executor.submit(\n",
    "                        writer.add_hparams(\n",
    "                            hparam_dict = {'lr': current_lr, 'bsize': batch_size, 'worker':worker_rank},\n",
    "                            metric_dict = {'correct': correct_t,'loss': loss_t.item()},\n",
    "                            name = 'eval-iter',\n",
    "                            global_step=t_count)\n",
    "                    )\n",
    "        \n",
    "        future = executor.submit(\n",
    "            writer.add_hparams(\n",
    "                hparam_dict = {'lr': current_lr, 'bsize': batch_size, 'worker':worker_rank},\n",
    "                metric_dict = {'correct': np.mean(agg_cor),'loss': np.mean(agg_loss), \n",
    "                               'last_correct': correct,'last_loss': loss.item()},\n",
    "                name = 'train',\n",
    "                global_step=epoch)\n",
    "        )\n",
    "\n",
    "        future = executor.submit(\n",
    "            writer.add_hparams(\n",
    "                hparam_dict = {'lr': current_lr, 'bsize': batch_size, 'worker':worker_rank},\n",
    "                metric_dict = {'correct': np.mean(agg_cor_t),'loss': np.mean(agg_loss_t), \n",
    "                               'last_correct': correct_t,'last_loss': loss_t.item()},\n",
    "                name = 'eval',\n",
    "                global_step=epoch)\n",
    "        )\n",
    "        \n",
    "        pickle.dump(model.state_dict(), s3.open(f\"pytorchtraining/pytorch_bigbatch/model_epoch{epoch}_iter{count}_{dt}.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### \n",
    "Now we've done all the hard work, and just need to run our function! Using `dispatch.run` from `dask-pytorch-ddp`, we pass in the transfer learning function so that it gets distributed correctly across our cluster. This creates futures and starts computing them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-23T16:55:49.034967Z",
     "iopub.status.busy": "2021-02-23T16:55:49.034816Z",
     "iopub.status.idle": "2021-02-23T16:55:50.679389Z",
     "shell.execute_reply": "2021-02-23T16:55:50.678920Z",
     "shell.execute_reply.started": "2021-02-23T16:55:49.034949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://d-steph-pytorch-training-90e6119a500640599ff558a22c25098d.main-namespace:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='https://d-steph-pytorch-training-90e6119a500640599ff558a22c25098d.internal.saturnenterprise.io' target='_blank'>https://d-steph-pytorch-training-90e6119a500640599ff558a22c25098d.internal.saturnenterprise.io</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>96</li>\n",
       "  <li><b>Memory: </b>382.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.0.10.44:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import datetime\n",
    "import json \n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_workers = 64\n",
    "\n",
    "s3 = s3fs.S3FileSystem()\n",
    "with s3.open('s3://saturn-public-data/dogs/imagenet1000_clsidx_to_labels.txt') as f:\n",
    "    imagenetclasses = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "client.restart() # Clears memory on cluster- optional but recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-23T16:55:50.680348Z",
     "iopub.status.busy": "2021-02-23T16:55:50.680175Z",
     "iopub.status.idle": "2021-02-23T16:55:50.683054Z",
     "shell.execute_reply": "2021-02-23T16:55:50.682594Z",
     "shell.execute_reply.started": "2021-02-23T16:55:50.680328Z"
    }
   },
   "outputs": [],
   "source": [
    "startparams = {'n_epochs': 6, \n",
    "                'batch_size': 100,\n",
    "                'train_pct': .8,\n",
    "                'base_lr': 0.01,\n",
    "                'imagenetclasses':imagenetclasses,\n",
    "                'subset': True,\n",
    "                'n_workers': 3} #only necessary if you select subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kick Off Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Tasks to Workers\n",
    " \n",
    "We talked in Notebook 2 about how we distribute tasks to the workers in our cluster, and now you get to see it firsthand. Inside the `dispatch.run()` function in `dask-pytorch-ddp`, we are actually using the `client.submit()` method to pass tasks to our workers, and collecting these as futures in a list. We can prove this by looking at the results, here named \"futures\", where we can see they are in fact all pending futures, one for each of the workers in our cluster.\n",
    "\n",
    "> *Why don't we use `.map()` in this function?*   \n",
    "> Recall that `.map` allows the Cluster to decide where the tasks are completed - it has the ability to choose which worker is assigned any task. That means that we don't have the control we need to ensure that we have one and only one job per GPU. This could be a problem for our methodology because of the use of DDP.    \n",
    "> Instead we use `.submit` and manually assign it to the workers by number. This way, each worker is attacking the same problem - our transfer learning problem - and pursuing a solution simultaneously. We'll have one and only one job per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-23T16:55:50.684410Z",
     "iopub.status.busy": "2021-02-23T16:55:50.684250Z",
     "iopub.status.idle": "2021-02-23T16:55:50.731589Z",
     "shell.execute_reply": "2021-02-23T16:55:50.731141Z",
     "shell.execute_reply.started": "2021-02-23T16:55:50.684391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.6 ms, sys: 435 µs, total: 40.1 ms\n",
      "Wall time: 40.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Future: pending, key: dispatch_with_ddp-98a39b1aca636983fb4acf1769934ef9>,\n",
       " <Future: pending, key: dispatch_with_ddp-923c7c9f12e5aa7ae181477a758e0a82>,\n",
       " <Future: pending, key: dispatch_with_ddp-9bc94b3cc8a7a75b8bf629084f1490fe>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time    \n",
    "futures = dispatch.run(client, run_transfer_learning, bucket = \"saturn-public-data\", prefix = \"dogs/Images\", **startparams)\n",
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-23T16:55:50.732522Z",
     "iopub.status.busy": "2021-02-23T16:55:50.732329Z",
     "iopub.status.idle": "2021-02-23T16:55:50.735454Z",
     "shell.execute_reply": "2021-02-23T16:55:50.735017Z",
     "shell.execute_reply.started": "2021-02-23T16:55:50.732502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Future: pending, key: dispatch_with_ddp-98a39b1aca636983fb4acf1769934ef9>,\n",
       " <Future: pending, key: dispatch_with_ddp-923c7c9f12e5aa7ae181477a758e0a82>,\n",
       " <Future: pending, key: dispatch_with_ddp-9bc94b3cc8a7a75b8bf629084f1490fe>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures[0].result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/VFDeGtRSHswfe/giphy.gif\" alt=\"parallel\" style=\"width: 200px;\"/>\n",
    "\n",
    "Now we let our workers run for awhile. This step will take time, so you may not be able to see the full results during our workshop. See the dashboards to view the GPUs efforts as the job runs.\n",
    "\n",
    "***\n",
    "\n",
    "If you don't have access to an S3 bucket, but would still like to do model performance review, please visit [Notebook 6b](06b-transfer-training-local.ipynb) and [Notebook 7](07-learning-results.ipynb) to see detailed examples of how you can do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Optional: Launch Tensorboard\n",
    "\n",
    "### If you save files to S3\n",
    "Open a terminal on your local machine, run `tensorboard --logdir=s3://[NAMEOFBUCKET]/runs`. Ensure that your AWS creds are in your bash profile/environment.\n",
    "\n",
    "#### Example of creds you should have\n",
    "export AWS_SECRET_ACCESS_KEY=`your secret key`   \n",
    "export AWS_ACCESS_KEY_ID=`your access key id`     \n",
    "export S3_REGION=us-east-2 `substitute your region`   \n",
    "export S3_ENDPOINT=https://s3.us-east-2.amazonaws.com `match to your region`   \n",
    "\n",
    "### If you save files locally\n",
    "\n",
    "When you are ready to start viewing the board, run this at the terminal inside Jupyter Labs:\n",
    "\n",
    "`tensorboard --logdir=runs`\n",
    "\n",
    "Then, in a terminal on your local machine, run: \n",
    "\n",
    "`ssh -L 6006:localhost:6006 -i ~/.ssh/PATHTOPRIVATEKEY SSHURLFORJUPYTER`\n",
    "\n",
    "You'll find the private key path on your local machine, and the SSH URL on the project page for this project. You can change the local port (the first 6006) if you like.\n",
    "\n",
    "At this stage, you'll likely not have any data, but the board will update itself every thirty seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
