{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/saturn_logo.png\" width=\"300\" />\n",
    "\n",
    "# Set Up Training\n",
    "\n",
    "We don't need to run all of Notebook 5 again, we'll just call `setup2.py` in the next chunk to get ourselves back to the right state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-12-03 22:45:06] INFO - dask-saturn | Cluster is ready\n",
      "[2020-12-03 22:45:06] INFO - dask-saturn | Registering default plugins\n",
      "[2020-12-03 22:45:06] INFO - dask-saturn | {'tcp://10.0.0.150:35343': {'status': 'repeat'}, 'tcp://10.0.24.234:35189': {'status': 'repeat'}, 'tcp://10.0.3.113:42823': {'status': 'repeat'}}\n"
     ]
    }
   ],
   "source": [
    "%run -i setup2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://d-steph-resnet-article-50680500b0454380ace50ab2e594ca29.main-namespace:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='https://d-steph-resnet-article-50680500b0454380ace50ab2e594ca29.internal.saturnenterprise.io' target='_blank'>https://d-steph-resnet-article-50680500b0454380ace50ab2e594ca29.internal.saturnenterprise.io</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>24</li>\n",
       "  <li><b>Memory: </b>181.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.0.15.222:8786' processes=3 threads=24, memory=181.50 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to do some learning! \n",
    "\n",
    "## Regular Model Details\n",
    "\n",
    "Aside from the Special Elements noted below, we can write this section essentially the same way we write any other PyTorch training loop. \n",
    "* Cross Entropy Loss for our loss function\n",
    "* SGD (Stochastic Gradient Descent) for our optimizer\n",
    "\n",
    "We're also using a particular learning rate scheduler called `ReduceLROnPlateau` which leaves our base learning rate alone until the model's efforts hit a plateau and the loss function is no longer decreasing.\n",
    "\n",
    "We have two stages in this process, as well - training and evaluation. We run the training set completely using batches of 100 before we move to the evaluation step, where we run the eval set completely also using batches of 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Special Elements\n",
    "\n",
    "Most of the training workflow function shown below is pretty standard for users of PyTorch. However, there are a couple of elements that are different.\n",
    "\n",
    "### DaskResultsHandler\n",
    "In order to use the model output handler, we need to initialize the `DaskResultsHandler` class for our experiment, from `dask-pytorch-ddp`.\n",
    "This object has a few important methods, including letting our model performance at each iteration be automatically documented.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "key = uuid.uuid4().hex\n",
    "\n",
    "rh = results.DaskResultsHandler(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Worker Rank\n",
    "```\n",
    "worker_rank = int(dist.get_rank())\n",
    "```\n",
    "\n",
    "This is checking to see which of the workers in the cluster we're on. This way, our results records can tell which worker this performance represents.\n",
    "\n",
    "\n",
    "### Model to Device\n",
    "\n",
    "```\n",
    "device = torch.device(0)\n",
    "net = models.resnet50(pretrained=True)\n",
    "model = net.to(device)\n",
    "```\n",
    "\n",
    "As you'll recall from Notebook 4, we need to make sure our model is placed on the worker- here we do it one time before the training loops begin. We will also pass each image and its label to the worker within the training and evaluation loops - see if you can find this spot, you need to fill in the blanks!\n",
    "\n",
    "\n",
    "### DDP Wrap\n",
    "```\n",
    "device_ids = [0]\n",
    "model = DDP(model, device_ids=device_ids)\n",
    "```\n",
    "\n",
    "And finally, we need to enable the DistributedDataParallel framework. To do this, we are using the `DDP()` wrapper around the model, which is short for the PyTorch function `torch.nn.parallel.DistributedDataParallel`. There is a lot to know about this, but for our purposes the important thing is to understand that this allows the model training to run in parallel on our cluster. https://pytorch.org/docs/stable/notes/ddp.html\n",
    "***\n",
    "\n",
    "## Discussing DDP\n",
    "It's helpful to know what this framework is really doing under the hood.\n",
    "\n",
    "The official PyTorch documentation tells us this:\n",
    "\n",
    "```This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.```\n",
    "\n",
    "Clear as mud, right? Let’s try to break it down.\n",
    "\n",
    "`This container parallelizes the application of the given module`\n",
    "\n",
    "This is just indicating that we’re parallelizing a deep learning workflow — transfer learning in our case.\n",
    "\n",
    "`by splitting the input across the specified devices by chunking in the batch dimension`\n",
    "\n",
    "Input for a transfer learning workflow is the dataset! Ok, so it is chunking our image batches and that’s what gets to be parallel.\n",
    "\n",
    "`(other objects will be copied once per device)`\n",
    "\n",
    "Eg, our starting model, if any (Resnet50 for us) doesn’t get broken up at all. Good to know.\n",
    "\n",
    "`In the forward pass, the module is replicated on each device, and each replica handles a portion of the input.`\n",
    "\n",
    "Ok, so the training task, our module, is replicated on each device. We have multiple copies of the job working simultaneously, and each one gets a chunk of the input images rather than the entire dataset.\n",
    "\n",
    "`During the backwards pass, gradients from each replica are summed into the original module.`\n",
    "\n",
    "And then each of these duplicate tasks passes the results (the gradients) back home to the original! The learning is happening out in the workers/child processes, and then they all return the results to the original module/training process to be aggregated.\n",
    "\n",
    "The essential difference with DDP, then, is that it is optimized for multiple machines instead of a single machine with multiple threads. It’s able to communicate across difference machines effectively, so we can use a GPU cluster for our computation.\n",
    "\n",
    "For a more detailed discussion and more tips about this same workflow, [you can visit our blog to read more!](https://www.saturncloud.io/s/combining-dask-and-pytorch-for-better-faster-transfer-learning/)\n",
    "\n",
    "***\n",
    "\n",
    "## Train that Baby!\n",
    "Our whole training process is going to be contained in one function, here named `run_transfer_learning`. There are some empty spots here referring to concepts we have discussed. Fill in the blanks in between `<<< >>>` marks to get the correct training function, or click the ellipsis below to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL IN THE BLANKS ###\n",
    "\n",
    "def run_transfer_learning(bucket, prefix, train_pct, batch_size, n_epochs, base_lr):\n",
    "    '''Load basic Resnet50, run transfer learning over given epochs.\n",
    "    Uses dataset from the path given as the pool from which to take the \n",
    "    training and evaluation samples.'''\n",
    "    \n",
    "    # --------- Format model and params --------- #\n",
    "    worker_rank = int(dist.get_rank())\n",
    "    \n",
    "    device = torch.device(0)\n",
    "    net = models.resnet50(pretrained=True)\n",
    "    model = net.to(<<< FILL IN >>>)\n",
    "    device_ids = [0]\n",
    "    model = <<< FILL IN >>>(model, device_ids=device_ids)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda()    \n",
    "    lr = base_lr * dist.get_world_size()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 2)\n",
    "    \n",
    "    # --------- Retrieve data for training and eval --------- #\n",
    "    whole_dataset = prepro_batches(bucket, prefix)\n",
    "    train, val = get_splits_parallel(train_pct, whole_dataset, batch_size=batch_size)\n",
    "    dataloaders = {'train' : train, 'val': val}\n",
    "\n",
    "    # --------- Start iterations --------- #\n",
    "    count = 0\n",
    "    t_count = 0\n",
    "    for epoch in range(n_epochs):\n",
    "    # --------- Training section --------- #    \n",
    "        model.train()  \n",
    "        for inputs, labels in dataloaders[\"train\"]:\n",
    "            dt = datetime.datetime.now().isoformat()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(<<< FILL IN >>>)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "\n",
    "            for param_group in optimizer.param_groups:\n",
    "                current_lr = param_group['lr']\n",
    "                \n",
    "            rh.submit_result(\n",
    "                f\"worker/{worker_rank}/data-{dt}.json\", \n",
    "                json.dumps({'loss': loss.item(),'learning_rate':current_lr, 'correct':correct, 'epoch': epoch, 'count': count, 'worker': worker_rank, 'sample': 'train'})\n",
    "            )\n",
    "        \n",
    "            if (count % 100) == 0 and worker_rank == 0:\n",
    "                rh.submit_result(f\"checkpoint-{dt}.pkl\", pickle.dumps(model.state_dict()))\n",
    "\n",
    "    # --------- Evaluation section --------- #   \n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            for inputs_t, labels_t in dataloaders[\"val\"]:\n",
    "                dt = datetime.datetime.now().isoformat()\n",
    "                inputs_t = inputs_t.to(<<< FILL IN >>>)\n",
    "                labels_t = labels_t.to(device)\n",
    "            \n",
    "                outputs_t = model(inputs_t)\n",
    "                _,pred_t = torch.max(outputs_t, dim=1)\n",
    "                loss_t = criterion(outputs_t, labels_t)\n",
    "                correct_t = (pred_t == labels_t).sum().item()\n",
    "                t_count += 1\n",
    "\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    current_lr = param_group['lr']\n",
    "                    \n",
    "                rh.submit_result(\n",
    "                    f\"worker/{worker_rank}/data-{dt}.json\", \n",
    "                    json.dumps({'loss': loss_t.item(),'learning_rate':current_lr, 'correct':correct_t, 'epoch': epoch, 'count': t_count, 'worker': worker_rank, 'sample': 'eval'})\n",
    "                )\n",
    "\n",
    "        scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_transfer_learning(bucket, prefix, train_pct, batch_size, n_epochs, base_lr):\n",
    "    '''Load basic Resnet50, run transfer learning over given epochs.\n",
    "    Uses dataset from the path given as the pool from which to take the \n",
    "    training and evaluation samples.'''\n",
    "    # --------- Format model and params --------- #\n",
    "    worker_rank = int(dist.get_rank())\n",
    "    \n",
    "    device = torch.device(0)\n",
    "    net = models.resnet50(pretrained=True)\n",
    "    model = net.to(device)\n",
    "    device_ids = [0]\n",
    "    model = DDP(model, device_ids=device_ids)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda()    \n",
    "    lr = base_lr * dist.get_world_size()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 2)\n",
    "    \n",
    "    # --------- Retrieve data for training and eval --------- #\n",
    "    whole_dataset = prepro_batches(bucket, prefix)\n",
    "    train, val = get_splits_parallel(train_pct, whole_dataset, batch_size=batch_size)\n",
    "    dataloaders = {'train' : train, 'val': val}\n",
    "\n",
    "    # --------- Start iterations --------- #\n",
    "    count = 0\n",
    "    t_count = 0\n",
    "    for epoch in range(n_epochs):\n",
    "    # --------- Training section --------- #    \n",
    "        model.train()  # Set model to training mode\n",
    "        for inputs, labels in dataloaders[\"train\"]:\n",
    "            dt = datetime.datetime.now().isoformat()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "            # statistics\n",
    "            for param_group in optimizer.param_groups:\n",
    "                current_lr = param_group['lr']\n",
    "            # Record the results of this model iteration (training sample) for later review.\n",
    "            rh.submit_result(\n",
    "                f\"worker/{worker_rank}/data-{dt}.json\", \n",
    "                json.dumps({'loss': loss.item(),'learning_rate':current_lr, 'correct':correct, 'epoch': epoch, 'count': count, 'worker': worker_rank, 'sample': 'train'})\n",
    "            )\n",
    "        \n",
    "            if (count % 100) == 0 and worker_rank == 0:\n",
    "                # Grab a snapshot of the current state of the model, in case of interruption or need to review\n",
    "                rh.submit_result(f\"checkpoint-{dt}.pkl\", pickle.dumps(model.state_dict()))\n",
    "\n",
    "    # --------- Evaluation section --------- #   \n",
    "        with torch.no_grad():\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            for inputs_t, labels_t in dataloaders[\"val\"]:\n",
    "                dt = datetime.datetime.now().isoformat()\n",
    "                inputs_t = inputs_t.to(device)\n",
    "                labels_t = labels_t.to(device)\n",
    "            \n",
    "                outputs_t = model(inputs_t)\n",
    "                _,pred_t = torch.max(outputs_t, dim=1)\n",
    "                loss_t = criterion(outputs_t, labels_t)\n",
    "                correct_t = (pred_t == labels_t).sum().item()\n",
    "                t_count += 1\n",
    "\n",
    "                # statistics\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    current_lr = param_group['lr']\n",
    "                # Record the results of this model iteration (evaluation sample) for later review.\n",
    "                rh.submit_result(\n",
    "                    f\"worker/{worker_rank}/data-{dt}.json\", \n",
    "                    json.dumps({'loss': loss_t.item(),'learning_rate':current_lr, 'correct':correct_t, 'epoch': epoch, 'count': t_count, 'worker': worker_rank, 'sample': 'eval'})\n",
    "                )\n",
    "\n",
    "        scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've done all the hard work, and just need to run our function! Using `dispatch.run` from `dask-pytorch-ddp`, we pass in the transfer learning function so that it gets distributed correctly across our cluster. This creates futures and starts computing them, while we use `process_results` to return the performance and learning statistics. \n",
    "\n",
    "### Define Model Parameters\n",
    "\n",
    "As with any PyTorch model, you'll want to define the epochs of training you plan to do, the batch size if using batches, and the starting learning rate. We're also able to assign the train/test split here because of how the functions above are written.\n",
    "\n",
    "(We're using only two epochs here to save time, but of course you can increase this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "startparams = {'n_epochs': 2, \n",
    "               'batch_size': 100,\n",
    "               'train_pct': .8,\n",
    "               'base_lr': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import datetime\n",
    "import json \n",
    "import pickle\n",
    "\n",
    "num_workers = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Tasks to Workers\n",
    " \n",
    "We talked in Notebook 2 about how we distribute tasks to the workers in our cluster, and now you get to see it firsthand. Inside the `dispatch.run()` function in `dask-pytorch-ddp`, we are actually using the `client.submit()` method to pass tasks to our workers, and collecting these as futures in a list. We can prove this by looking at the results, here named \"futures\", where we can see they are in fact all pending futures, one for each of the workers in our cluster.\n",
    "\n",
    "Remember, each worker is attacking the same problem - our transfer learning problem - and pursuing a solution simultaneously. We'll have one and only one job per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 ms, sys: 0 ns, total: 26.6 ms\n",
      "Wall time: 25.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Future: pending, key: dispatch_with_ddp-9e0964d3c0f27c2de7be58c9eb7e9a38>,\n",
       " <Future: pending, key: dispatch_with_ddp-e559c9da599c666b647fc482d3d58282>,\n",
       " <Future: pending, key: dispatch_with_ddp-1da294f872ab79b9c84bbb23e2ed0468>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time    \n",
    "futures = dispatch.run(client, run_transfer_learning, bucket = \"saturn-public-data\", prefix = \"dogs/Images\", **startparams)\n",
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<b>GPU Dashboard links</b>\n",
       "<ul>\n",
       "<li><a href=\"https://d-steph-resnet-article-50680500b0454380ace50ab2e594ca29.internal.saturnenterprise.io/individual-gpu-memory\" target=\"_blank\">GPU memory</a></li>\n",
       "<li><a href=\"https://d-steph-resnet-article-50680500b0454380ace50ab2e594ca29.internal.saturnenterprise.io/individual-gpu-utilization\" target=\"_blank\">GPU utilization</a></li>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(gpu_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/VFDeGtRSHswfe/giphy.gif\" alt=\"parallel\" style=\"width: 200px;\"/>\n",
    "\n",
    "Now we let our workers run for awhile. This step will take time, so you may not be able to see the full results during our workshop. (In tests, it took about 13 minutes to do two epochs.) See the two links above to view the GPUs efforts as the job runs. This won't hold up your Jupyter environment here, but I promise the cluster is working hard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Results\n",
    "\n",
    "This step is where we gather up and save the results. While the cluster is working away at the computation, we can run the `process_results()` method on the DaskResultsHandler. This will be us requesting the results of each future as they run, which is familiar from [Notebook 4](04-parallel-inference.ipynb), where we used `fut.result()`. To see partial results coming in, you should have the `workshop_results` folder in the folder menu a few moments after you run the next two chunks. Look in this folder to see the results each worker is returning to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/jovyan/project/workshop-dask-pytorch/workshop_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rh.process_results(\"/home/jovyan/project/workshop-dask-pytorch/workshop_results\", futures, raise_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will continue to hold up your Jupyter instance until it has been able to collect all the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Results\n",
    "\n",
    "We don't have the time today to run an assortment of different cluster sizes to see what works best, but I happen to have the results of those runs saved and visualized, to demonstrate how well it works! [Follow me to Notebook 7!](07-learning-results.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
