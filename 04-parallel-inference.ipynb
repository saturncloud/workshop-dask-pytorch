{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/saturn_logo.png\" width=\"300\" />\n",
    "\n",
    "# Parallel Inference\n",
    "\n",
    "We are ready to scale up our inference task!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/4H5nOUqX7FywOGpCF7/giphy.gif\" alt=\"scaleup\" style=\"width: 200px;\"/>\n",
    "\n",
    "We've done this before, but to refresh your memory, get connected to the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SaturnCluster(\n",
    "    n_workers = 3, \n",
    "    scheduler_size = 'medium', \n",
    "    worker_size = 'p32xlarge',\n",
    "    nthreads = 8\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(3)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i setup1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the command above to get ourselves back to the state we need from Notebook 3.\n",
    "\n",
    "***\n",
    "\n",
    "## Assigning Objects to GPU Resources\n",
    "\n",
    "If you are going to run any processes on GPU resources in a cluster, you need all your objects to be explicitly told this. Otherwise, it won't seek out GPU resources. However, if you use a functional setup (as we are going to do later) you'll need to do this INSIDE your function. Our architecture below will have all that written in. But before we go too complex, we should learn how that works in isolation.\n",
    "\n",
    "This command is all you need to assign an object (a model, an image, etc) to a GPU-type resource. [The PyTorch docs can tell us more.](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device) So here's how we do it with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you try. What would you write to assign a transformed image (call it `img_t`) to a GPU resource? We'll do this a few more times in the upcoming examples.\n",
    "\n",
    "Fill in the blanks in between `<<< >>>` marks to get the correct code, or click the ellipsis below to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL IN THE BLANKS ###\n",
    "\n",
    "img_t = <<< FILL IN >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "img_t = img_t.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Images\n",
    "\n",
    "Our goal here is to create a nicely streamlined workflow, including loading, transforming, batching, and labeling images, which we can then run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "@dask.delayed\n",
    "def preprocess(path, fs=__builtins__):\n",
    "    '''Ingest images directly from S3, apply transformations,\n",
    "    and extract the ground truth and image identifier. Accepts\n",
    "    a filepath. '''\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256), \n",
    "        transforms.CenterCrop(250), \n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    with fs.open(path, 'rb') as f:\n",
    "        img = Image.open(f).convert(\"RGB\")\n",
    "        nvis = transform(img)\n",
    "\n",
    "    truth = re.search('dogs/Images/n[0-9]+-([^/]+)/n[0-9]+_[0-9]+.jpg', path).group(1)\n",
    "    name = re.search('dogs/Images/n[0-9]+-[a-zA-Z-_]+/(n[0-9]+_[0-9]+).jpg', path).group(1)\n",
    "    \n",
    "    return [name, nvis, truth]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does a number of things for us.\n",
    "* Open an image file from S3\n",
    "* Apply transformations to image\n",
    "* Retrieve a unique identifier for the image\n",
    "* Retrieve the ground truth label for the image\n",
    "\n",
    "But you'll notice that this has a `@dask.delayed` decorator, so we can queue it without it running immediately when called. Because of this, we can use some list comprehension strategies to create our batches and get them ready for our inference.\n",
    "\n",
    "First, we break the list of images we have from our S3 filepath into chunks that will define the batches. (We defined `s3` when we connected to the S3 file storage in [Notebook 3](03-single-inference.ipynb), if you forgot.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toolz\n",
    "\n",
    "s3fpath = 's3://saturn-public-data/dogs/Images/*/*.jpg'\n",
    "batch_breaks = [list(batch) for batch in toolz.partition_all(60, s3.glob(s3fpath))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does one of our batches look like? It's a list of image paths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_breaks[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batches = [[preprocess(x, fs=s3) for x in y] for y in batch_breaks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def reformat(batch):\n",
    "    flat_list = [item for item in batch]\n",
    "    tensors = [x[1] for x in flat_list]\n",
    "    names = [x[0] for x in flat_list]\n",
    "    labels = [x[2] for x in flat_list]\n",
    "    return [names, tensors, labels]\n",
    "    \n",
    "image_batches = [reformat(result) for result in image_batches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get a nice visual representation of the tasks we have queued up, we can use the `.visualize()` method on a delayed object, like this. We've set up a lot of tasks in this one batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batches[0].visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our images ready! But as you know, we really just have a list of tasks queued up that we're going to ask our cluster to complete later.\n",
    "\n",
    "***\n",
    "\n",
    "## Run the Model\n",
    "We are ready to do the inference task! This is going to have a few steps, all of which are contained in functions described below, but we’ll talk through them so everything is clear, using just one batch as an example.\n",
    "\n",
    "Our unit of work at this point is batches of 60 images at a time, which we created in the section above. They are all neatly arranged in lists so that we can work with them effectively.\n",
    "\n",
    "### Stack Tensors\n",
    "One thing we need to do with the lists is to “stack” the tensors. We could do this earlier in our process, but because we are using the Dask delayed decorator on the preprocessing, our functions actually do not know that they are receiving tensors until later in the process. Therefore, we’re delaying the “stacking” as well by putting it inside this function that comes after the preprocessing.\n",
    "\n",
    "\n",
    "### One-Batch Example\n",
    "In this next bit, we'll step through the elements of our inference function for one batch, before we do the scaled-up job.\n",
    "\n",
    "First: snag one batch of our images to use as a test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import re\n",
    "iteritem = image_batches[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with s3.open('s3://saturn-public-data/dogs/imagenet1000_clsidx_to_labels.txt') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "names, images, truelabels = iteritem\n",
    "\n",
    "images = torch.stack(images) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our tensors stacked so that batches can be passed to the model. We are going to retrieve our model, using syntax that will be very familiar from [Notebook 3](03-single-inference.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "images = images.to(device)\n",
    "pred_batch = resnet(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign our image stack (just the batch we are working on) to the GPU and then run the inference, returning predictions for that batch.\n",
    "\n",
    "***\n",
    "\n",
    "## Result Evaluation\n",
    "\n",
    "The predictions and truth we have so far, however, are not really human readable or comparable, so we’ll use the functions that follow to fix them up and get us interpretable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pred_batch(batch, gtruth, classes):\n",
    "    ''' Accepts batch of images, returns human readable predictions. '''\n",
    "    _, indices = torch.sort(batch, descending=True)\n",
    "    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100\n",
    "    \n",
    "    preds = []\n",
    "    labslist = []\n",
    "    for i in range(len(batch)):\n",
    "        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]\n",
    "        preds.append(pred)\n",
    "\n",
    "        labs = gtruth[i]\n",
    "        labslist.append(labs)\n",
    "        \n",
    "    return(preds, labslist)\n",
    "\n",
    "preds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes our results from the model, and a few other elements, to return nice readable predictions and the probabilities the model assigned. From here, we’re nearly done! We want to pass our results back to S3 in a tidy, human readable way, so the rest of the function handles that. It will iterate over each image because these functionalities are not batch handling. `is_match` is one of our custom functions, which you can check out below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def is_match(la, ev):\n",
    "    ''' Evaluate human readable prediction against ground truth. \n",
    "    (Used in both methods)'''\n",
    "    if re.search(la.replace('_', ' '), str(ev).replace('_', ' ')):\n",
    "        match = True\n",
    "    else:\n",
    "        match = False\n",
    "    return(match)\n",
    "\n",
    "for j in range(0, len(images)):\n",
    "    predicted = preds[j]\n",
    "    groundtruth = labslist[j]\n",
    "    name = names[j]\n",
    "    match = is_match(groundtruth, predicted)\n",
    "\n",
    "    outcome = {'name': name, 'ground_truth': groundtruth, 'prediction': predicted, 'evaluation': match}\n",
    "\n",
    "# Write each result to S3 directly - not active in this demo, this is only an example of how you could do it.\n",
    "#     with s3.open(f\"s3://fake-results-bucket/dogs/preds/{name}.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(outcome, f)\n",
    "        \n",
    "print(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go, that's one result from our batch.\n",
    "\n",
    "## Put It All Together\n",
    "\n",
    "Now, we aren’t going to patch together all these computations by hand, instead we have assembled them in one single delayed function that will do the work for us. Importantly, we can then map this across all our batches of images across the cluster! Can you spot all the tasks we have described above? \n",
    "\n",
    "Fill in the blanks in between `<<< >>>` marks to get the correct code, or click the ellipsis below to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL IN THE BLANKS ###\n",
    "\n",
    "def evaluate_pred_batch(batch, gtruth, classes):\n",
    "    ''' Accepts batch of images, returns human readable predictions. '''\n",
    "    _, indices = torch.sort(batch, descending=True)\n",
    "    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100\n",
    "    \n",
    "    preds = []\n",
    "    labslist = []\n",
    "    for i in range(len(batch)):\n",
    "        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]\n",
    "        preds.append(pred)\n",
    "\n",
    "        labs = gtruth[i]\n",
    "        labslist.append(labs)\n",
    "        \n",
    "    return(preds, labslist)\n",
    "\n",
    "def is_match(la, ev):\n",
    "    ''' Evaluate human readable prediction against ground truth. \n",
    "    (Used in both methods)'''\n",
    "    if re.search(la.replace('_', ' '), str(ev).replace('_', ' ')):\n",
    "        match = True\n",
    "    else:\n",
    "        match = False\n",
    "    return(match)    \n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def run_batch_to_s3(iteritem):\n",
    "    ''' Accepts iterable result of preprocessing, \n",
    "    generates inferences and evaluates. '''\n",
    "    \n",
    "    with s3.open('s3://saturn-public-data/dogs/imagenet1000_clsidx_to_labels.txt') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "  \n",
    "    names, images, truelabels = iteritem\n",
    "    \n",
    "    images = torch.<<< FILL IN >>>(images)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Set up model\n",
    "        resnet = models.<<< FILL IN >>>(pretrained=True)\n",
    "        resnet = resnet.to(device)\n",
    "        resnet.eval()\n",
    "\n",
    "        # run model on batch\n",
    "        images = images.to(<<< FILL IN >>>)\n",
    "        pred_batch = resnet(images)\n",
    "        \n",
    "        #Evaluate batch\n",
    "        preds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)\n",
    "\n",
    "        #Organize prediction results\n",
    "        for j in range(0, len(images)):\n",
    "            predicted = preds[j]\n",
    "            groundtruth = labslist[j]\n",
    "            name = names[j]\n",
    "            match = is_match(groundtruth, predicted)\n",
    "            \n",
    "            outcome = {'name': name, 'ground_truth': groundtruth, 'prediction': predicted, 'evaluation': match}\n",
    "\n",
    "        return(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_pred_batch(batch, gtruth, classes):\n",
    "    ''' Accepts batch of images, returns human readable predictions. '''\n",
    "    _, indices = torch.sort(batch, descending=True)\n",
    "    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100\n",
    "    \n",
    "    preds = []\n",
    "    labslist = []\n",
    "    for i in range(len(batch)):\n",
    "        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]\n",
    "        preds.append(pred)\n",
    "\n",
    "        labs = gtruth[i]\n",
    "        labslist.append(labs)\n",
    "        \n",
    "    return(preds, labslist)\n",
    "\n",
    "def is_match(la, ev):\n",
    "    ''' Evaluate human readable prediction against ground truth. \n",
    "    (Used in both methods)'''\n",
    "    if re.search(la.replace('_', ' '), str(ev).replace('_', ' ')):\n",
    "        match = True\n",
    "    else:\n",
    "        match = False\n",
    "    return(match)    \n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def run_batch_to_s3(iteritem):\n",
    "    ''' Accepts iterable result of preprocessing, \n",
    "    generates inferences and evaluates. '''\n",
    "    \n",
    "    with s3.open('s3://saturn-public-data/dogs/imagenet1000_clsidx_to_labels.txt') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "  \n",
    "    names, images, truelabels = iteritem\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Set up model\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        resnet = resnet.to(device)\n",
    "        resnet.eval()\n",
    "\n",
    "        # run model on batch\n",
    "        images = images.to(device)\n",
    "        pred_batch = resnet(images)\n",
    "        \n",
    "        #Evaluate batch\n",
    "        preds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)\n",
    "\n",
    "        #Organize prediction results\n",
    "        for j in range(0, len(images)):\n",
    "            predicted = preds[j]\n",
    "            groundtruth = labslist[j]\n",
    "            name = names[j]\n",
    "            match = is_match(groundtruth, predicted)\n",
    "            \n",
    "            outcome = {'name': name, 'ground_truth': groundtruth, 'prediction': predicted, 'evaluation': match}\n",
    "\n",
    "        return(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Job\n",
    "\n",
    "If you think you've filled in everything correctly, now you can try running the tasks in parallel. If you get errors, check the hidden chunk for answers.\n",
    "\n",
    "Notice that we're going to use client methods below to ensure that our tasks are distributed across the cluster, run, and then retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(gpu_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "futures = client.map(run_batch_to_s3, image_batches) \n",
    "futures_gathered = client.gather(futures)\n",
    "futures_computed = client.compute(futures_gathered, sync=False)\n",
    "\n",
    "import logging\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "for fut in futures_computed:\n",
    "    try:\n",
    "        result = fut.result()\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "        logging.error(e)\n",
    "    else:\n",
    "        results.extend(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this block, we might want to go visit the Dask dashboard, to see our work as it runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* You can apply `@dask.delayed` to your custom code to allow parallelization with nearly zero refactoring\n",
    "* Objects that are needed for a parallel task on GPU need to be assigned to a GPU resource\n",
    "* Passing tasks to the workers uses mapping across the cluster for peak efficiency\n",
    "\n",
    "And, of course, having multiple workers makes the job a lot faster!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/Ood1OSF92jubS/giphy.gif\" alt=\"parallel\" style=\"width: 250px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
