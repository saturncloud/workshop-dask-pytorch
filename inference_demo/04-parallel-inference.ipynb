{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/saturn_logo.png\" width=\"300\" />\n",
    "\n",
    "# Parallel Inference\n",
    "\n",
    "We are ready to scale up our inference task!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/4H5nOUqX7FywOGpCF7/giphy.gif\" alt=\"scaleup\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "**Dataset:** [Stanford Dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/main.html)  \n",
    "**Model:** [Resnet50](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "\n",
    "We've done this before, but to refresh your memory, get connected to the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "from torchvision import datasets, transforms, models\n",
    "import re\n",
    "\n",
    "cluster = SaturnCluster()\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(3)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../tools/setup1.py\n",
    "\n",
    "display(HTML(gpu_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the command above to get ourselves back to the state we need from Notebook 3.\n",
    "\n",
    "***\n",
    "\n",
    "## Assigning Objects to GPU Resources\n",
    "\n",
    "If you are going to run any processes on GPU resources in a cluster, you need all your objects to be explicitly told this. Otherwise, it won't seek out GPU resources. However, if you use a functional setup (as we are going to do later) you'll need to do this INSIDE your function. Our architecture below will have all that written in. But before we go too complex, we should learn how that works in isolation.\n",
    "\n",
    "This command is all you need to assign an object (a model, an image, etc) to a GPU-type resource. [The PyTorch docs can tell us more.](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device) So here's how we do it with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would you write to assign a transformed image (call it `img_t`) to a GPU resource? \n",
    "We'll do this a few more times in the upcoming examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = img_t.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Images\n",
    "\n",
    "Our goal here is to create a nicely streamlined workflow, including loading, transforming, batching, and labeling images, which we can then run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "@dask.delayed\n",
    "def preprocess(path, fs=__builtins__):\n",
    "    '''Ingest images directly from S3, apply transformations,\n",
    "    and extract the ground truth and image identifier. Accepts\n",
    "    a filepath. '''\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256), \n",
    "        transforms.CenterCrop(250), \n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    with fs.open(path, 'rb') as f:\n",
    "        img = Image.open(f).convert(\"RGB\")\n",
    "        nvis = transform(img)\n",
    "\n",
    "    truth = re.search('dogs/Images/n[0-9]+-([^/]+)/n[0-9]+_[0-9]+.jpg', path).group(1)\n",
    "    name = re.search('dogs/Images/n[0-9]+-[a-zA-Z-_]+/(n[0-9]+_[0-9]+).jpg', path).group(1)\n",
    "    \n",
    "    return [name, nvis, truth]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does a number of things for us.\n",
    "* Open an image file from S3\n",
    "* Apply transformations to image\n",
    "* Retrieve a unique identifier for the image\n",
    "* Retrieve the ground truth label for the image\n",
    "\n",
    "But you'll notice that this has a `@dask.delayed` decorator, so we can queue it without it running immediately when called. Because of this, we can use some list comprehension strategies to create our batches and get them ready for our inference.\n",
    "\n",
    "First, we break the list of images we have from our S3 filepath into chunks that will define the batches. (We defined `s3` when we connected to the S3 file storage in [Notebook 3](03-single-inference.ipynb), if you forgot.)\n",
    "\n",
    "***\n",
    "\n",
    "### List Image Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toolz\n",
    "\n",
    "s3fpath = 's3://saturn-public-data/dogs/Images/*/*.jpg'\n",
    "batch_breaks = [list(batch) for batch in toolz.partition_all(80, s3.glob(s3fpath))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_breaks[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does one of our batches look like? It's a list of image paths!\n",
    "\n",
    "***\n",
    "\n",
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batches = [[preprocess(x, fs=s3) for x in y] for y in batch_breaks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batches[0][0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat\n",
    "\n",
    "<img src=\"../img/batch-reformatting.png\" width = 700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def reformat(batch):\n",
    "    flat_list = [item for item in batch]\n",
    "    tensors = [x[1] for x in flat_list]\n",
    "    names = [x[0] for x in flat_list]\n",
    "    labels = [x[2] for x in flat_list]\n",
    "    \n",
    "    tensors = torch.stack(tensors).to(device)\n",
    "    \n",
    "    return [names, tensors, labels]\n",
    "\n",
    "image_batches = [reformat(result) for result in image_batches] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get a nice visual representation of the tasks we have queued up, we can use the `.visualize()` method on a delayed object, like this. We've set up a lot of tasks in this one batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batches[0].visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our images ready! But as you know, we really just have a list of tasks queued up that we're going to ask our cluster to complete later.\n",
    "\n",
    "***\n",
    "\n",
    "## Check Images\n",
    "\n",
    "### Image Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = image_batches[25][0][:5].compute()\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batches[25][2][:5].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cpudevice = torch.device(\"cpu\")\n",
    "tensorset = image_batches[25].compute()\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "imglist = [to_pil(tensorset[1][0].to(cpudevice)), \n",
    "           to_pil(tensorset[1][1].to(cpudevice)),\n",
    "           to_pil(tensorset[1][2].to(cpudevice)),\n",
    "           to_pil(tensorset[1][3].to(cpudevice)),\n",
    "           to_pil(tensorset[1][4].to(cpudevice))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 5, figsize=(16,6))\n",
    "\n",
    "for i in range(0,5):\n",
    "    img1 = imglist[i]\n",
    "    ax[i].imshow(img1).axes.xaxis.set_visible(False)\n",
    "    ax[i].axes.yaxis.set_visible(False)\n",
    "\n",
    "title = 'Sample Images'\n",
    "f.suptitle(title, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "## Run the Model\n",
    "We are ready to do the inference task! This is going to have a few steps, all of which are contained in functions described below, but we’ll talk through them so everything is clear, using just one batch as an example.\n",
    "\n",
    "Our unit of work at this point is batches of 60 images at a time, which we created in the section above. They are all neatly arranged in lists so that we can work with them effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "## Result Evaluation\n",
    "\n",
    "The predictions and truth we have so far, however, are not really human readable or comparable, so we’ll use the functions that follow to fix them up and get us interpretable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pred_batch(batch, gtruth, classes):\n",
    "    ''' Accepts batch of images, returns human readable predictions. '''\n",
    "    \n",
    "    _, indices = torch.sort(batch, descending=True)\n",
    "    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100\n",
    "    \n",
    "    preds = []\n",
    "    labslist = []\n",
    "    for i in range(len(batch)):\n",
    "        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]\n",
    "        preds.append(pred)\n",
    "\n",
    "        labs = gtruth[i]\n",
    "        labslist.append(labs)\n",
    "        \n",
    "    return(preds, labslist)\n",
    "\n",
    "def is_match(label, pred):\n",
    "    ''' Evaluates human readable prediction against ground truth.'''\n",
    "    if re.search(label.replace('_', ' '), str(pred).replace('_', ' ')):\n",
    "        match = True\n",
    "    else:\n",
    "        match = False\n",
    "    return(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes our results from the model, and a few other elements, to return nice readable predictions and the probabilities the model assigned. From here, we’re nearly done! We want to pass our results back to S3 in a tidy, human readable way, so the rest of the function handles that. It will iterate over each image because these functionalities are not batch handling. `is_match` is one of our custom functions, which you can check out below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Put It All Together\n",
    "\n",
    "Now, we aren’t going to patch together all these computations by hand, instead we have assembled them in one single delayed function that will do the work for us. Importantly, we can then map this across all our batches of images across the cluster! Can you spot all the tasks we have described above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def run_batch_to_s3(iteritem):\n",
    "    ''' Accepts iterable result of preprocessing, generates\n",
    "    inferences and evaluates. '''\n",
    "  \n",
    "    names, images, truelabels = iteritem\n",
    "    \n",
    "    with s3.open('s3://saturn-public-data/dogs/imagenet1000_clsidx_to_labels.txt') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    # Retrieve, set up model\n",
    "    resnet = models.resnet50(pretrained=True)\n",
    "    resnet = resnet.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        resnet.eval()\n",
    "        pred_batch = resnet(images)\n",
    "        \n",
    "        #Evaluate batch\n",
    "        preds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)\n",
    "\n",
    "        #Organize prediction results\n",
    "        outcomes = []\n",
    "        for j in range(0, len(images)):\n",
    "            match = is_match(labslist[j], preds[j])            \n",
    "            outcome = {'name': names[j], 'ground_truth': labslist[j], \n",
    "                       'prediction': preds[j], 'evaluation': match}\n",
    "            outcomes.append(outcome)\n",
    "    \n",
    "        return(outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Job\n",
    "\n",
    "If you think you've filled in everything correctly, now you can try running the tasks in parallel. If you get errors, check the hidden chunk for answers.\n",
    "\n",
    "Notice that we're going to use client methods below to ensure that our tasks are distributed across the cluster, run, and then retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(gpu_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "futures = client.map(run_batch_to_s3, image_batches) \n",
    "futures_gathered = client.gather(futures)\n",
    "futures_computed = client.compute(futures_gathered, sync=False)\n",
    "\n",
    "import logging\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "for fut in futures_computed:\n",
    "    try:\n",
    "        result = fut.result()\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "        logging.error(e)\n",
    "    else:\n",
    "        results.extend(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this block, we might want to go visit the Dask dashboard, to see our work as it runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Review Results\n",
    "\n",
    "Look at the graph for one batch, and spot check output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = run_batch_to_s3(image_batches[0])\n",
    "test_sample.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures_computed[0].result()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Original Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dogs = [d for d in results if d['name'] in test_set]\n",
    "test_dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 5, figsize=(16,6))\n",
    "\n",
    "for i in range(0,5):\n",
    "    img1 = imglist[i]\n",
    "    ax[i].imshow(img1).axes.xaxis.set_visible(False)\n",
    "    ax[i].axes.yaxis.set_visible(False)\n",
    "\n",
    "title = 'Sample Images'\n",
    "f.suptitle(title, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_preds = [x['evaluation'] for x in results if x['evaluation'] == True]\n",
    "false_preds = [x['evaluation'] for x in results if x['evaluation'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_preds)/len(results)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Lessons Learned\n",
    "\n",
    "* You can apply `@dask.delayed` to your custom code to allow parallelization with nearly zero refactoring\n",
    "* Objects that are needed for a parallel task on GPU need to be assigned to a GPU resource\n",
    "* Passing tasks to the workers uses mapping across the cluster for peak efficiency\n",
    "\n",
    "And, of course, having multiple workers makes the job a lot faster!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/Ood1OSF92jubS/giphy.gif\" alt=\"parallel\" style=\"width: 250px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
