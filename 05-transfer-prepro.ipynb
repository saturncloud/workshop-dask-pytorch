{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/saturn_logo.png\" width=\"300\" />\n",
    "\n",
    "\n",
    "# Transfer Learning\n",
    "\n",
    "After Notebooks 3 and 4, we know how to run a very speedy inference job with our parallelization from Dask. But what if we need to train a model? Let's do a transfer learning task to see how that might work.\n",
    "\n",
    "We are still using Stanford Dogs and starting with Resnet50, and we will use transfer learning to make it perform better at dog image identification.\n",
    "\n",
    "In order to make this work, we have a few steps to carry out:\n",
    "* Preprocessing our data appropriately\n",
    "* Applying infrastructure for parallelizing the learning process\n",
    "* Running the transfer learning workflow and generating evaluation data\n",
    "\n",
    "\n",
    "To start, you know the drill by now: get our cluster connected. Fill in the blanks in between `<<< >>>` marks to get the correct code, or click the ellipsis below to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "import s3fs\n",
    "import re\n",
    "from torchvision import transforms\n",
    "\n",
    "cluster = SaturnCluster()\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(3)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Preprocessing Data\n",
    "\n",
    "We are using `dask-pytorch-ddp` to handle a lot of the work involved in training across the entire cluster. This will abstract away lots of worker management tasks, and also sets up a tidy infrastructure for managing model output, but if you're interested to learn more about this, we maintain the [codebase and documentation on Github](https://github.com/saturncloud/dask-pytorch).\n",
    "\n",
    "Because we want to load our images directly from S3, without saving them to memory (and wasting space/time!) we are going to use the `dask-pytorch-ddp` custom class inheriting from the Dataset class called `S3ImageFolder`.\n",
    "\n",
    "The preprocessing steps are quite short- we want to load images using the class we discussed above, and apply the transformation of our choosing. If you like, you can make the transformations an argument to this function and pass it in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_pytorch_ddp import results, data, dispatch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_batches(bucket, prefix):\n",
    "    '''Initialize the custom Dataset class defined above, apply transformations.'''\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize(256), \n",
    "    transforms.CenterCrop(250), \n",
    "    transforms.ToTensor()])\n",
    "    whole_dataset = data.S3ImageFolder(\n",
    "        bucket, \n",
    "        prefix, \n",
    "        transform=transform, \n",
    "        anon = True\n",
    "    )\n",
    "    return whole_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Data Labels\n",
    "\n",
    "Because our task is transfer learning, we're going to be starting with the pretrained Resnet50 model. In order to take full advantage of the training that the model already has, we need to make sure that the label indices on our Stanford Dogs dataset match their equivalents in the Resnet50 label data. (Hint: they aren't going to match, but we'll fix it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "with s3.open('s3://saturn-public-data/dogs/imagenet1000_clsidx_to_labels.txt') as f:\n",
    "    imagenetclasses = [line.strip() for line in f.readlines()]\n",
    "\n",
    "whole_dataset = prepro_batches(bucket = \"saturn-public-data\", prefix = \"dogs/Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset loaded in a PyTorch image folder object will have a few attributes, including `class_to_idx` which returns a dictionary of the class names and their assigned indices. Let's look at the one for our dog images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(whole_dataset.class_to_idx.items())[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's look at the Imagenet classes - do they match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenetclasses[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's not going to work! Our model thinks 1 = goldfish while our dataset thinks 1 = Japanese Spaniel. Fortunately, this is a pretty easy fix. \n",
    "\n",
    "I've created a function called `replace_label()` that checks the labels by text with regex, so that we can be assured that we match them up correctly. This is important, because we can't assume all our dog labels are in exactly the same consecutive order in the imagenet labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_label(dataset_label, model_labels):\n",
    "    label_string = re.search('n[0-9]+-([^/]+)', dataset_label).group(1)\n",
    "    \n",
    "    for i in model_labels:\n",
    "        i = str(i).replace('{', '').replace('}', '')\n",
    "        model_label_str = re.search('''b[\"'][0-9]+: [\"']([^\\/]+)[\"'],[\"']''', str(i))\n",
    "        model_label_idx = re.search('''b[\"']([0-9]+):''', str(i)).group(1)\n",
    "        \n",
    "        if re.search(str(label_string).replace('_', ' '), str(model_label_str).replace('_', ' ')):\n",
    "            return i, model_label_idx\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function in a couple of lines of list comprehension to create our new `class_to_idx` object. Now we have the indices assigned to match our imagenet dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_class_to_idx = {x: int(replace_label(x, imagenetclasses)[1]) for x in whole_dataset.classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(new_class_to_idx.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenetclasses[151:156]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make sure our old and new datasets have the same length, so that nothing got missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_class_to_idx.items()) == len(whole_dataset.class_to_idx.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Select Training and Evaluation Samples\n",
    "\n",
    "In order to run our training, we'll create training and evaluation sample sets to use later. These generate DataLoader objects which we can iterate over. We'll use both later to run and monitor our model's learning.\n",
    "\n",
    "Note the `multiprocessing_context` argument that we are using in the DataLoader objects - this is important! It will allow our large batch jobs to load more than one image simultaneously, and save us a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits_parallel(train_pct, data, batch_size):\n",
    "    '''Select two samples of data for training and evaluation'''\n",
    "    classes = data.classes\n",
    "    train_size = math.floor(len(data) * train_pct)\n",
    "    indices = list(range(len(data)))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx = indices[:train_size]\n",
    "    test_idx = indices[train_size:len(data)]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        data, \n",
    "        sampler=train_sampler,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        multiprocessing_context=mp.get_context('fork'))\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        data, \n",
    "        sampler=train_sampler, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers, \n",
    "        multiprocessing_context=mp.get_context('fork'))\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from using our custom data object, this should be very similar to other PyTorch workflows. While I am using the `S3ImageFolder` class here, you definitely don't have to in your own work. Any standard PyTorch data object type should be compatible with the Dask work we're doing next.\n",
    "\n",
    "Now, it's time for learning, in [Notebook 6a](06a-transfer-training-s3.ipynb)!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/mC7VjtF9sYofs9DUa5/giphy.gif\" alt=\"learn\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
