{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/saturn_logo.png\" width=\"300\" />\n",
    "\n",
    "\n",
    "# Transfer Learning\n",
    "\n",
    "Now we know how to run a very speedy inference job with our parallelization from Dask. But what if we need to train a model? Let's do a transfer learning task to see how that might work.\n",
    "\n",
    "We are still using Stanford Dogs and starting with Resnet50, and we will use transfer learning to make it perform better at dog image identification.\n",
    "\n",
    "In order to make this work, we have a few steps to carry out:\n",
    "* Preprocessing our data appropriately\n",
    "* Applying infrastructure for parallelizing the learning process\n",
    "* Running the transfer learning workflow and generating evaluation data\n",
    "\n",
    "\n",
    "To start, you know the drill by now: get our cluster connected. Fill in the blanks in between `<<< >>>` marks to get the correct code, or click the ellipsis below to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL IN THE BLANKS ###\n",
    "\n",
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = <<< FILL IN >>>(\n",
    "    n_workers = 3, \n",
    "    scheduler_size = 'g4dn4xlarge', \n",
    "    worker_size = 'g4dn8xlarge'\n",
    ")\n",
    "client = <<< FILL IN >>>(cluster)\n",
    "client.wait_for_workers(3)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SaturnCluster(\n",
    "    n_workers = 3, \n",
    "    scheduler_size = 'g4dn4xlarge', \n",
    "    worker_size = 'g4dn8xlarge'\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(3)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Preprocessing Data\n",
    "\n",
    "We are using `dask-pytorch-ddp` to handle a lot of the work involved in training across the entire cluster. This will abstract away lots of worker management tasks, and also sets up a tidy infrastructure for managing model output, but if you're interested to learn more about this, we maintain the [codebase and documentation on Github](https://github.com/saturncloud/dask-pytorch).\n",
    "\n",
    "Because we want to load our images directly from S3, without saving them to memory (and wasting space/time!) we are going to use the `dask-pytorch-ddp` custom class inheriting from the Dataset class called `S3ImageFolder`.\n",
    "\n",
    "The preprocessing steps are quite short- we want to load images using the class we discussed above, and apply the transformation of our choosing. If you like, you can make the transformations an argument to this function and pass it in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_pytorch_ddp import results, data, dispatch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_batches(bucket, prefix):\n",
    "    '''Initialize the custom Dataset class defined above, apply transformations.'''\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize(256), \n",
    "    transforms.CenterCrop(250), \n",
    "    transforms.ToTensor()])\n",
    "    whole_dataset = data.S3ImageFolder(bucket, prefix, transform=transform)\n",
    "    return whole_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Training and Evaluation Samples\n",
    "\n",
    "In order to run our training, we'll create training and evaluation sample sets to use later. These generate DataLoader objects which we can iterate over. We'll use both later to run and monitor our model's learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits_parallel(train_pct, data, batch_size):\n",
    "    '''Select two samples of data for training and evaluation'''\n",
    "    classes = data.classes\n",
    "    train_size = math.floor(len(data) * train_pct)\n",
    "    indices = list(range(len(data)))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx = indices[:train_size]\n",
    "    test_idx = indices[train_size:len(data)]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(data, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers, multiprocessing_context=mp.get_context('fork'))\n",
    "    test_loader = torch.utils.data.DataLoader(data, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers, multiprocessing_context=mp.get_context('fork'))\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from using our custom data object, this should be very similar to other PyTorch workflows. While I am using the `S3ImageFolder` class here, you definitely don't have to in your own work. Any standard PyTorch data object type should be compatible with the Dask work we're doing next.\n",
    "\n",
    "Now, it's time for learning!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/mC7VjtF9sYofs9DUa5/giphy.gif\" alt=\"learn\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
